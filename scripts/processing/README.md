## Creating the tagged PBC

This folder contains scripts that support various aspects of the processes used to create the tagged PBC. Each script is outlined briefly below.

`PBC_parseBibles.py` is a modified version of the script used to extract a range of verses from translations of the Bible as found in the PBC. This version additionally loads word tokenizers for specific scripts/languages and does romanization. Due to copyright restrictions and other considerations the original files are not included here, and only the result of the tagging process (the corpus underpinning the current paper's findings) is made available (under the "corpora/" folder in the main repository). As a result, this script will not run as-is.

`get_indices.py` compares two different English translations of the NT and lemmatizes them, then uses the lemmatized version to select verses from the NT. Due to copyright restrictions, the original translations are not included here, though it is debatable whether they could be included under "fair use", since the lemmatization essentially represents a different kind of text than the original translation. This script therefore will not run without issue, though I have included its output lists of shared verbs/nouns and indices in the `scripts/data/translation/` folder of the main repository. The indices generated from the script are the verses of the NT used for training machine translation models.

`train_models.py` is the script used to train each **IBM Model 2** word alignment model to translate from one language (source) to English (target). These models are then used to tag words in each set of source language verses based on the POS tag of the English translation equivalent in the respective verse. As this depends on the PBC, similar restrictions apply. Given that the resulting models and tokenizers are collectively 40GB, they are also not included as part of this repository.

`reformat_treebanks.py` is a script used to convert the UDT data to a format for comparison with the tagged PBC. It assumes that you have converted your UD-2.14 data to ISO 639-3 codes using the `create_iso_639_3_symlinks.py` script provided in the "ud-tools-2.14" folder of the UDT download. The script essentially combines all UDT data for each represented ISO and converts it to sentences containing tuples in the format (word, POS). Some of these datasets were additionally romanized, using similar code to that found in `PBC_parseBibles.py`.